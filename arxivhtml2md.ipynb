{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f74fed62",
   "metadata": {},
   "source": [
    "# arXiv HTML to Markdown Converter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d3bd71",
   "metadata": {},
   "source": [
    "Converts arXiv papers to markdown by fetching HTML rendered via LaTeXML. Tries **ar5iv.org** first, then falls back to **arxiv.org/html** (useful for brand-new papers not yet on ar5iv). Both sources use identical HTML structure, so the conversion works seamlessly. The `alttext` attribute on `<math>` elements contains the original LaTeX.\n",
    "\n",
    "## Quick Start (via Tools)\n",
    "\n",
    "The recommended way to use this is via the arXiv tools (see \"arXiv Paper Tools\" section above). These support **multiple papers** in a single session with **automatic session persistence**:\n",
    "\n",
    "1. **Fetch papers:** `arxiv_fetch(\"1706.03762\")` — adds TOC as a note, stores paper for later\n",
    "2. **List loaded papers:** `arxiv_list()` — shows all papers with abstracts for disambiguation\n",
    "3. **Get sections:** `arxiv_section(\"3\", \"1706.03762\")` — adds section as a note\n",
    "4. **Get all sections:** `arxiv_all_sections(\"1706.03762\")` — adds all sections as notes\n",
    "5. **Unload paper:** `arxiv_remove(\"1706.03762\")` — removes from store\n",
    "\n",
    "## Session Persistence\n",
    "\n",
    "Paper IDs are saved to `.arxiv_session.json`. After a kernel restart:\n",
    "- **`arxiv_list()`** — silently reloads all papers from the session file\n",
    "- **`arxiv_section()`** / **`arxiv_all_sections()`** — auto-refetches if needed, with a \"(Re-fetched ... after kernel restart)\" message\n",
    "\n",
    "## Manual Usage (Direct Class)\n",
    "\n",
    "```python\n",
    "# Create converter from arXiv ID or URL\n",
    "conv = Ar5ivToMarkdown(\"1706.03762\").fetch()\n",
    "\n",
    "# Check which source was used (ar5iv.org or arxiv.org)\n",
    "print(f\"Fetched from: {conv.source_base}\")\n",
    "\n",
    "# Browse the table of contents\n",
    "print(conv.toc())\n",
    "\n",
    "# Access sections programmatically\n",
    "conv.sections  # List of (index, num, title, content)\n",
    "\n",
    "# Get full markdown\n",
    "md = conv.convert()\n",
    "\n",
    "# Add sections as dialog notes\n",
    "conv.add_section_by_num(3)          # Section 3\n",
    "conv.add_section_by_num(\"A.1\")      # Appendix A.1\n",
    "conv.add_section_by_num(\"Abstract\") # By title\n",
    "conv.add_all_sections()             # All sections\n",
    "```\n",
    "\n",
    "## Dependencies\n",
    "`httpx`, `beautifulsoup4`, `lxml`, `html2text`\n",
    "\n",
    "## Current Limitations\n",
    "- Complex tables may not render perfectly\n",
    "- Author parsing assumes `&` or `\\AND` separators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666a9719",
   "metadata": {},
   "source": [
    "# Core Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7220d1",
   "metadata": {
    "time_run": "2026-02-21T16:36:48.888341+00:00"
   },
   "outputs": [],
   "source": [
    "from dialoghelper import add_msg\n",
    "import re\n",
    "import httpx\n",
    "import html2text\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "class Ar5ivToMarkdown:\n",
    "    \"\"\"Convert ar5iv HTML papers to markdown\"\"\"\n",
    "    \n",
    "    def __init__(self, arxiv_id):\n",
    "        if 'arxiv.org' in arxiv_id or 'ar5iv.org' in arxiv_id:\n",
    "            arxiv_id = re.search(r'(\\d+\\.\\d+)', arxiv_id).group(1)\n",
    "        self.arxiv_id = arxiv_id\n",
    "        self.source_base = \"https://ar5iv.org\"\n",
    "        self.soup = None\n",
    "        self._sections = None\n",
    "        \n",
    "    def fetch(self):\n",
    "        \"\"\"Fetch and parse - tries ar5iv first, falls back to arxiv.org/html\"\"\"\n",
    "        for base in [\"https://ar5iv.org\", \"https://arxiv.org\"]:\n",
    "            response = httpx.get(f\"{base}/html/{self.arxiv_id}\", follow_redirects=True)\n",
    "            if response.status_code == 200 and '<article' in response.text:\n",
    "                self.soup = BeautifulSoup(response.text, 'lxml')\n",
    "                self.source_base = base\n",
    "                return self\n",
    "        raise ValueError(f\"Could not fetch {self.arxiv_id} from ar5iv or arxiv\")\n",
    "    \n",
    "    def _parse_authors(self, authors_div):\n",
    "        text = authors_div.get_text()\n",
    "        text = re.sub(r'\\\\AND', '&', text)\n",
    "        text = re.sub(r'\\d*footnotemark:?\\s*\\d*', '', text)\n",
    "        authors = []\n",
    "        for a in text.split('&'):\n",
    "            lines = [l.strip() for l in a.strip().split('\\n') if l.strip()]\n",
    "            if lines:\n",
    "                authors.append({\n",
    "                    'name': lines[0], \n",
    "                    'affiliation': lines[1] if len(lines) > 1 else '', \n",
    "                    'email': lines[2] if len(lines) > 2 else ''\n",
    "                })\n",
    "        return authors\n",
    "    \n",
    "    def _format_authors(self, authors):\n",
    "        by_affil = defaultdict(list)\n",
    "        for a in authors:\n",
    "            affil = a['affiliation'] if a['affiliation'] and '@' not in a['affiliation'] else 'Independent'\n",
    "            by_affil[affil].append(a['name'])\n",
    "        lines = [\"**Authors:**\\n\"]\n",
    "        for affil, names in by_affil.items():\n",
    "            lines.append(f\"*{affil}:* {', '.join(names)}\")\n",
    "        return '\\n'.join(lines)\n",
    "    \n",
    "    def _preprocess(self, article):\n",
    "        # Handle equation tables FIRST (before individual math tags)\n",
    "        # Normalize image alt text to avoid bracket issues\n",
    "        for img in article.find_all('img'):\n",
    "            alt = img.get('alt', '')\n",
    "            if alt:\n",
    "                img['alt'] = alt.strip('[]')\n",
    "        \n",
    "\n",
    "        # Convert span-based tables to real tables (LaTeXML sometimes uses spans)\n",
    "        for span_table in article.find_all('span', class_='ltx_tabular'):\n",
    "            span_table.name = 'table'\n",
    "            for row in span_table.find_all('span', class_='ltx_tr'):\n",
    "                row.name = 'tr'\n",
    "            for cell in span_table.find_all('span', class_='ltx_td'):\n",
    "                cell.name = 'td'\n",
    "\n",
    "\n",
    "        for table in article.find_all('table', class_='ltx_equationgroup'):\n",
    "            rows = table.find_all('tr', class_='ltx_equation')\n",
    "            result_lines = []\n",
    "            for row in rows:\n",
    "                latex_parts = []\n",
    "                for math in row.find_all('math'):\n",
    "                    alt = math.get('alttext', '')\n",
    "                    alt = re.sub(r'^\\\\displaystyle\\s*', '', alt)\n",
    "                    alt = re.sub(r'^\\\\\\[|\\\\\\]$', '', alt)  # Remove \\[ and \\]\n",
    "                    alt = re.sub(r'^\\\\\\(|\\\\\\)$', '', alt)  # Remove \\( and \\)\n",
    "                    if alt:\n",
    "                        latex_parts.append(alt)\n",
    "                eqno_cell = row.find('td', class_='ltx_eqn_eqno')\n",
    "                eqno = eqno_cell.get_text(strip=True) if eqno_cell else ''\n",
    "                if latex_parts:\n",
    "                    latex = ' '.join(latex_parts)\n",
    "                    if eqno and (m := re.match(r'\\((\\d+)\\)', eqno)):\n",
    "                        latex += f\" \\\\tag{{{m.group(1)}}}\"\n",
    "                    result_lines.append(f\"$${latex}$$\")\n",
    "            table.replace_with(BeautifulSoup(f'<p>{chr(10).join(result_lines)}</p>', 'lxml'))\n",
    "        \n",
    "        authors_div = article.find('div', class_='ltx_authors')\n",
    "        if authors_div:\n",
    "            md = self._format_authors(self._parse_authors(authors_div))\n",
    "            authors_div.replace_with(BeautifulSoup(f'<p>{md}</p>', 'lxml'))\n",
    "        \n",
    "        abstract = article.find('div', class_='ltx_abstract')\n",
    "        if abstract and (h6 := abstract.find('h6')):\n",
    "            h6.name = 'h2'\n",
    "        \n",
    "        for math in list(article.find_all('math')):\n",
    "            alt = math.get('alttext', '')\n",
    "            if not alt: continue  # skip if no LaTeX available\n",
    "            # Strip any existing delimiters from alttext\n",
    "            alt = re.sub(r'^\\\\displaystyle\\s*', '', alt)\n",
    "            alt = re.sub(r'^\\\\\\[|\\\\\\]$', '', alt)  # Remove \\[ and \\]\n",
    "            alt = re.sub(r'^\\\\\\(|\\\\\\)$', '', alt)  # Remove \\( and \\)\n",
    "            repl = f'$${alt}$$' if math.get('display') == 'block' else f'${alt}$'\n",
    "            math.replace_with(repl)\n",
    "    \n",
    "    def convert(self):\n",
    "        \"\"\"Convert paper to markdown\"\"\"\n",
    "        if not self.soup: self.fetch()\n",
    "        article = self.soup.find('article')\n",
    "        self._preprocess(article)\n",
    "        \n",
    "        h = html2text.HTML2Text()\n",
    "        h.body_width = 0\n",
    "        md = h.handle(str(article))\n",
    "        \n",
    "        # Fix any remaining \\[...\\] or \\(...\\) delimiters that slipped through\n",
    "        md = re.sub(r'\\\\\\[', '$$', md)\n",
    "        md = re.sub(r'\\\\\\]', '$$', md)\n",
    "        md = re.sub(r'\\\\\\(', '$', md)\n",
    "        md = re.sub(r'\\\\\\)', '$', md)\n",
    "        \n",
    "        md = re.sub(r'<(https?://[^>]+)>', r'[\\1](\\1)', md)\n",
    "        md = re.sub(r'!\\[([^\\]]*)\\]\\((?!http)([^)]+)\\)', rf'![\\1]({self.source_base}\\2)', md)\n",
    "        md = md.replace('\\\\\\\\', '\\\\')\n",
    "        md = md.replace('\\\\eqqcolon', '=:')\n",
    "        # Fix \\bigg{[} → \\bigg[ (KaTeX doesn't support braced delimiters)\n",
    "        md = re.sub(r'\\\\([Bb]ig+)\\{([[\\]()])\\}', r'\\\\\\1\\2', md)\n",
    "        \n",
    "        return md\n",
    "    \n",
    "    @property\n",
    "    def title(self):\n",
    "        if not self.soup: self.fetch()\n",
    "        h1 = self.soup.find('h1', class_='ltx_title')\n",
    "        if h1:\n",
    "            return h1.get_text(strip=True)\n",
    "        bold = self.soup.find('span', class_='ltx_font_bold')\n",
    "        if bold:\n",
    "            return bold.get_text(strip=True)\n",
    "        return \"Untitled\"\n",
    "    \n",
    "    def _section_title(self, section):\n",
    "        if m := re.match(r'^##\\s*(.+?)$', section, re.MULTILINE):\n",
    "            raw = m.group(1).strip()\n",
    "            if num_match := re.match(r'^(\\d+(?:\\.\\d+)*|[A-Z](?:\\.\\d+)*)\\s+(.+)$', raw):\n",
    "                return num_match.group(1), num_match.group(2)\n",
    "            return None, raw\n",
    "        return None, \"Preamble\"\n",
    "    \n",
    "    @property \n",
    "    def sections(self):\n",
    "        \"\"\"Get list of (index, section_num, title, content) for each section\"\"\"\n",
    "        if self._sections is None:\n",
    "            md = self.convert()\n",
    "            parts = re.split(r'(?=^## )', md, flags=re.MULTILINE)\n",
    "            self._sections = [(i, *self._section_title(s), s.strip()) \n",
    "                             for i, s in enumerate(parts) if s.strip()]\n",
    "        return self._sections\n",
    "    \n",
    "    def get_section(self, num_or_title):\n",
    "        \"\"\"Get section by number (e.g., '3', 'A.1') or title (e.g., 'Abstract')\"\"\"\n",
    "        for sec in self.sections:\n",
    "            if sec[1] == str(num_or_title):  # match by number\n",
    "                return sec\n",
    "        # fallback: match by title (case-insensitive)\n",
    "        for sec in self.sections:\n",
    "            if num_or_title.lower() in sec[2].lower():\n",
    "                return sec\n",
    "        return None\n",
    "    \n",
    "    def toc(self):\n",
    "        \"\"\"Return table of contents as markdown string\"\"\"\n",
    "        lines = [f\"# {self.title}\", f\"**arXiv:** {self.arxiv_id}\\n\"]\n",
    "        for i, num, title, _ in self.sections:\n",
    "            prefix = f\"{num}.\" if num else \"-\"\n",
    "            lines.append(f\"{prefix} {title}\")\n",
    "        return '\\n'.join(lines)\n",
    "    \n",
    "    async def add_toc_note(self):\n",
    "        \"\"\"Add TOC as a note message\"\"\"\n",
    "        return await add_msg(content=self.toc(), placement=\"at_end\")\n",
    "    \n",
    "    async def add_section_note(self, index):\n",
    "        \"\"\"Add a single section by list index\"\"\"\n",
    "        _, _, _, content = self.sections[index]\n",
    "        return await add_msg(content=content, placement=\"at_end\")\n",
    "    \n",
    "    async def add_section_by_num(self, num):\n",
    "        \"\"\"Add section by paper number (e.g., '3', 'A.1')\"\"\"\n",
    "        sec = self.get_section(num)\n",
    "        if sec: return await add_msg(content=sec[3], placement=\"at_end\")\n",
    "        return None\n",
    "    \n",
    "    async def add_all_sections(self):\n",
    "        \"Add all sections as separate note messages\"\n",
    "        for _, _, _, content in self.sections: await add_msg(content=content, placement=\"at_end\")\n",
    "\n",
    "async def arxiv_toc(url_or_id: str) -> Ar5ivToMarkdown:\n",
    "    \"\"\"Quick TOC from URL or ID — adds note and returns converter for further use\"\"\"\n",
    "    conv = Ar5ivToMarkdown(url_or_id).fetch()\n",
    "    await conv.add_toc_note()\n",
    "    return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3add2e4",
   "metadata": {},
   "source": [
    "## How the Ar5iv Converter Works\n",
    "\n",
    "The `Ar5ivToMarkdown` class converts arXiv papers to markdown by leveraging LaTeXML-rendered HTML. It tries **ar5iv.org** first, then falls back to **arxiv.org/html** for papers not yet available on ar5iv (e.g., brand-new submissions).\n",
    "\n",
    "### Key Steps\n",
    "\n",
    "1. **Fetch**: Tries `ar5iv.org/html/{arxiv_id}` first; if unavailable, falls back to `arxiv.org/html/{arxiv_id}`. Stores which source was used in `self.source_base` for correct image URL handling.\n",
    "\n",
    "2. **Preprocess**: Before conversion, it:\n",
    "   - Extracts author info and reformats it cleanly (grouping by affiliation)\n",
    "   - Converts `<math>` tags back to LaTeX using their `alttext` attribute (e.g., `<math alttext=\"\\alpha\">` → `$\\alpha$`)\n",
    "   - Promotes the abstract heading from `<h6>` to `<h2>`\n",
    "\n",
    "3. **Convert**: Uses `html2text` to transform the preprocessed HTML into markdown, then fixes relative image URLs\n",
    "\n",
    "4. **Parse sections**: Splits the markdown at `## ` headers, extracts section numbers (like \"3.1\" or \"A.2\") and titles\n",
    "\n",
    "### Helper Functions\n",
    "\n",
    "- `toc()` — Returns a table of contents\n",
    "- `get_section(num)` — Retrieves a section by its paper number\n",
    "- `add_*` methods — Create note messages in the dialog\n",
    "\n",
    "The `arxiv_toc()` convenience function does fetch + add TOC in one call, returning the converter for further use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8089f53",
   "metadata": {},
   "source": [
    "# Tool Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb9b1e",
   "metadata": {
    "time_run": "2026-02-21T16:36:49.091206+00:00"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "_arxiv_papers = {}  # {arxiv_id: Ar5ivToMarkdown}\n",
    "_SESSION_FILE = Path(\".arxiv_session.json\")\n",
    "\n",
    "def _save_session():\n",
    "    \"\"\"Save current paper IDs to disk\"\"\"\n",
    "    _SESSION_FILE.write_text(json.dumps(list(_arxiv_papers.keys())))\n",
    "\n",
    "def _load_session():\n",
    "    \"\"\"Load paper IDs from disk\"\"\"\n",
    "    if _SESSION_FILE.exists():\n",
    "        return set(json.loads(_SESSION_FILE.read_text()))\n",
    "    return set()\n",
    "\n",
    "def _ensure_paper(paper_id):\n",
    "    \"\"\"Load paper from memory, or re-fetch if in session file. Returns (conv, refetched_msg)\"\"\"\n",
    "    if paper_id in _arxiv_papers:\n",
    "        return _arxiv_papers[paper_id], \"\"\n",
    "    \n",
    "    if paper_id in _load_session():\n",
    "        conv = Ar5ivToMarkdown(paper_id).fetch()\n",
    "        _arxiv_papers[paper_id] = conv\n",
    "        return conv, f\" (Re-fetched {paper_id} after kernel restart)\"\n",
    "    \n",
    "    return None, \"\"\n",
    "\n",
    "async def arxiv_fetch(url_or_id: str) -> str:\n",
    "    \"\"\"Fetch an arXiv paper and return its table of contents.\n",
    "    \n",
    "    Args:\n",
    "        url_or_id: arXiv URL (arxiv.org or ar5iv.org) or just the ID (e.g., '1706.03762')\n",
    "    \n",
    "    Returns:\n",
    "        Table of contents with title, arXiv ID, and section listing\n",
    "    \"\"\"\n",
    "    conv = Ar5ivToMarkdown(url_or_id).fetch()\n",
    "    _arxiv_papers[conv.arxiv_id] = conv\n",
    "    _save_session()\n",
    "    await add_msg(content=conv.toc(), placement=\"at_end\")\n",
    "    return conv.toc()\n",
    "\n",
    "async def arxiv_list() -> str:\n",
    "    \"\"\"List all currently loaded arXiv papers with abstracts for disambiguation.\n",
    "    \n",
    "    Returns:\n",
    "        Formatted list of paper IDs, titles, and abstract snippets\n",
    "    \"\"\"\n",
    "    # If no papers in memory, try to reload from session file\n",
    "    if not _arxiv_papers:\n",
    "        session_ids = _load_session()\n",
    "        for paper_id in session_ids:\n",
    "            conv = Ar5ivToMarkdown(paper_id).fetch()\n",
    "            _arxiv_papers[paper_id] = conv\n",
    "    \n",
    "    if not _arxiv_papers:\n",
    "        return \"No papers loaded. Use arxiv_fetch first.\"\n",
    "    \n",
    "    lines = [\"**Loaded papers:**\"]\n",
    "    for pid, conv in _arxiv_papers.items():\n",
    "        abstract = conv.get_section(\"Abstract\")\n",
    "        abstract_text = abstract[3][:300] + \"...\" if abstract else \"(no abstract)\"\n",
    "        lines.append(f\"- **{pid}**: {conv.title}\\n  {abstract_text}\")\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "async def arxiv_section(num: str, paper_id: str) -> str:\n",
    "    \"\"\"Get a specific section from a loaded arXiv paper.\n",
    "    \n",
    "    Args:\n",
    "        num: Section number (e.g., '3', '4.1', 'A.1') or title (e.g., 'Abstract')\n",
    "        paper_id: arXiv ID of the paper (e.g., '1706.03762')\n",
    "    \n",
    "    Returns:\n",
    "        The section content as markdown\n",
    "    \"\"\"\n",
    "    conv, refetch_msg = _ensure_paper(paper_id)\n",
    "    if not conv:\n",
    "        return f\"Paper '{paper_id}' not loaded. Use arxiv_fetch first.\"\n",
    "    \n",
    "    sec = conv.get_section(num)\n",
    "    if sec:\n",
    "        await add_msg(content=sec[3], placement=\"at_end\")\n",
    "        return f\"Added section {num}: {sec[2]}{refetch_msg}\"\n",
    "    return f\"Section '{num}' not found in {paper_id}\"\n",
    "\n",
    "async def arxiv_all_sections(paper_id: str) -> str:\n",
    "    \"\"\"Get all sections from a loaded arXiv paper.\n",
    "    \n",
    "    Args:\n",
    "        paper_id: arXiv ID of the paper (e.g., '1706.03762')\n",
    "    \n",
    "    Returns:\n",
    "        Confirmation of sections added\n",
    "    \"\"\"\n",
    "    conv, refetch_msg = _ensure_paper(paper_id)\n",
    "    if not conv:\n",
    "        return f\"Paper '{paper_id}' not loaded. Use arxiv_fetch first.\"\n",
    "    \n",
    "    for _, _, _, content in conv.sections: await add_msg(content=content, placement=\"at_end\")\n",
    "    return f\"Added {len(conv.sections)} sections from {paper_id}{refetch_msg}\"\n",
    "\n",
    "def arxiv_remove(paper_id: str) -> str:\n",
    "    \"\"\"Remove a paper from the loaded papers store.\n",
    "    \n",
    "    Args:\n",
    "        paper_id: arXiv ID of the paper to remove (e.g., '1706.03762')\n",
    "    \n",
    "    Returns:\n",
    "        Confirmation of removal\n",
    "    \"\"\"\n",
    "    if paper_id not in _arxiv_papers:\n",
    "        return f\"Paper '{paper_id}' not loaded.\"\n",
    "    \n",
    "    title = _arxiv_papers[paper_id].title\n",
    "    del _arxiv_papers[paper_id]\n",
    "    _save_session()\n",
    "    return f\"Removed {paper_id}: {title}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6334f602",
   "metadata": {},
   "source": [
    "## arXiv Paper Tools\n",
    "\n",
    "Use these tools to fetch and read arXiv papers (all tools add notes to the dialog):\n",
    "\n",
    "- &`arxiv_fetch` — Fetch a paper by URL or ID, adds TOC as a note, stores paper for later access\n",
    "- &`arxiv_list` — List all loaded papers with IDs, titles, and abstract snippets (auto-reloads from session file after kernel restart)\n",
    "- &`arxiv_section` — Add a specific section by number (e.g., \"3\", \"A.1\") or title (e.g., \"Abstract\"); requires `paper_id`\n",
    "- &`arxiv_all_sections` — Add all sections as separate notes; requires `paper_id`\n",
    "- &`arxiv_remove` — Unload a paper from the store (also removes from session file)\n",
    "\n",
    "**Session persistence:** Paper IDs are saved to `.arxiv_session.json` in the current working directory (i.e., per-folder, not at the CRAFT root), so papers survive kernel restarts and are auto-reloaded on demand. This means each project folder maintains its own paper session.\n",
    "\n",
    "**Example workflow:** Fetch one or more papers, use `arxiv_list` to see what's loaded, then retrieve specific sections by paper ID."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac4f470",
   "metadata": {},
   "source": [
    "## Version History\n",
    "\n",
    "### v1.2 — 2026-02-21\n",
    "- Fixed missing `await` on all `add_msg` calls in `Ar5ivToMarkdown` methods and tool functions; marked all affected methods/functions `async`\n",
    "\n",
    "### v1.1 — 2026-02-21\n",
    "- Fixed section ordering in `arxiv_all_sections`, `arxiv_section`, `arxiv_fetch`, and all `Ar5ivToMarkdown.add_*` methods: changed `add_msg` calls from default `placement=\"add_after\"` (which caused reverse insertion order) to `placement=\"at_end\"` so notes appear in paper section order.\n",
    "\n",
    "### v1.0 — 2026-01-31\n",
    "- Initial implementation of `Ar5ivToMarkdown` with ar5iv → arxiv.org fallback\n",
    "- LaTeX math reconstruction from `alttext` attributes\n",
    "- Author grouping by affiliation\n",
    "- Section parsing and TOC generation\n",
    "- `arxiv_fetch`, `arxiv_list`, `arxiv_section`, `arxiv_all_sections`, `arxiv_remove` tool functions\n",
    "- Session persistence via `.arxiv_session.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422650e7",
   "metadata": {},
   "source": [
    "# Example Manual Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f60590",
   "metadata": {
    "time_run": "2026-02-21T16:36:49.010324+00:00"
   },
   "outputs": [],
   "source": [
    "# Quick TOC from URL\n",
    "#conv = await arxiv_toc(\"https://arxiv.org/abs/1706.03762\")\n",
    "\n",
    "# Add a specific section (by index from TOC)\n",
    "#await conv.add_section_by_num(4)\n",
    "\n",
    "# Or add all sections at once\n",
    "#await conv.add_all_sections()"
   ]
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "concise",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
